# -*- coding: utf-8 -*-
"""LinearRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QN2tDJNM856x0MFsZguwIHlOn8vE-3UT

We will construct a linear model that explains the relationship a car's mileage (mpg) has with its other attributes

## Import Libraries
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from sklearn.linear_model import LinearRegression
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function

"""## Load and review data"""

cData = pd.read_csv("/content/drive/MyDrive/SOCR-HeightWeight.csv")
cData.shape

#data frame of the data
cData.head()

#data type and memory information
cData.info()

cData.describe()

cData.isnull().sum()

# droping unwanted column
cData=cData.drop('Index',axis=1)

#dataframe of the data
cData.tail()

"""## BiVariate Plots

A bivariate analysis among the different variables can be done using scatter matrix plot. Seaborn libs create a dashboard reflecting useful information about the dimensions. The result can be stored as a .png file.
"""

cData_attr = cData.iloc[:, 0:7]
sns.pairplot(cData_attr, diag_kind='kde')   # to plot density curve instead of histogram on the diag

# distribution
cData.hist(bins=10)

#corelation between columns
cData.corr()

"""Observation between 'mpg' and other attributes indicate the relationship is not really linear. However, the plots also indicate that linearity would still capture quite a bit of useful information/pattern. Several assumptions of classical linear regression seem to be violated, including the assumption of no Heteroscedasticity

## Split Data
"""

# lets build our linear model
# independant variables
X = cData['Height(Inches)']
# the dependent variable
y = cData[['Weight(Pounds)']]

X.head()

y.head()

X.shape

y.shape

# Split X and y into training and test set in 70:30 ratio

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

#converting one dim array to two dim array
import pandas as pd

# Convert the Series object to a DataFrame object
X_train = pd.DataFrame(X_train)

# Convert the DataFrame to a NumPy array
X_train = X_train.values

# Reshape the NumPy array
X_train = X_train.reshape(-1, 1)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""## Fit Linear Model"""

regression_model = LinearRegression()
regression_model.fit(X_train, y_train)

regression_model.intercept_

regression_model.coef_

output = ((-0.39480797)*8) + ((0.02894551)*307) + ((-0.02175221)*130) + ((-0.00735203)*3504 )+ ((0.06191937)*12 )+ ((0.83693389)*70) + ((-3.001283)*1) + ((-0.60601796)*0) -18.28345112

output

16.70-15.182178009999998

"""Here are the coefficients for each variable and the intercept

The score (R^2) for in-sample and out of sample
"""

regression_model.score(X_train, y_train)

#converting one dim array to two dim array
# Convert the Series object to a DataFrame object
X_test = pd.DataFrame(X_train)

# Convert the DataFrame to a NumPy array
X_test = X_test.values

# Reshape the NumPy array
X_test = X_test.reshape(-1, 1)

regression_model.score(X_test, y_test)

"""## Adding interaction terms

"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model

poly = PolynomialFeatures(degree=2, interaction_only=True)
X_train2 = poly.fit_transform(X_train)
X_test2 = poly.fit_transform(X_test)

poly_clf = linear_model.LinearRegression()

poly_clf.fit(X_train2, y_train)

y_pred = poly_clf.predict(X_test2)

#print(y_pred)

#In sample (training) R^2 will always improve with the number of variables!
print(poly_clf.score(X_train2, y_train))

#Out off sample (testing) R^2 is our measure of sucess and does improve
print(poly_clf.score(X_test2, y_test))

# but this improves as the cost of 29 extra variables!
print(X_train.shape)
print(X_train2.shape)

"""Polynomial Features (with only interaction terms) have improved the Out of sample R^2. However at the cost of increaing the number of variables significantly.

"""

# prompt: predict the Weight(Pounds) by getting height as input

import numpy as np
height = float(input("Enter height in inches: "))
X_new = np.array([height]).reshape(-1, 1)
X_new2 = poly.fit_transform(X_new)
predicted_weight = poly_clf.predict(X_new2)
print("Predicted Weight (Pounds):", predicted_weight[0][0])